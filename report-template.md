# Report: Predict Bike Sharing Demand with AutoGluon Solution
#### Hamza Farhat

## Initial Training
### What did you realize when you tried to submit your predictions? What changes were needed to the output of the predictor to submit your results?
When submitting the predictions, it was necessary to ensure that all predicted values were non-negative.
Kaggle would reject any submission where the values in the prediction column were less than zero. As a result,
I applied a transformation to set any negative predictions to zero.

### What was the top ranked model that performed?
The highest-ranked model in the initial training was "WeightedEnsemble_L3."

## Exploratory data analysis and feature creation
### What did the exploratory analysis find and how did you add additional features?
The exploratory data analysis revealed that features like time of day, weather conditions,
and temperature were significant contributors to bike demand. I added features such as:
- Hour of the day
- Day of the week
- Whether it was a holiday or not
- Weather conditions
These additional features helped the model capture trends in bike usage that vary based on time and environmental factors.

### How much better did your model preform after adding additional features and why do you think that is?
After adding these new features, the model's performance improved, particularly in terms of accuracy and RMSE. 
This improvement likely resulted from the model being better able to capture seasonal and temporal patterns in the bike-sharing data. 
Features like "hour" and "day of the week" helped the model better account for rush hours and weekends, 
where bike-sharing demand is generally higher.

## Hyper parameter tuning
### How much better did your model preform after trying different hyper parameters?
After tuning the hyperparameters, I saw an increase in performance compared to the default settings. 
Specific parameters like learning_rate and max_depth were adjusted for models like XGBoost, improving prediction accuracy. 
However, the gains from hyperparameter optimization were more modest compared to adding features.

### If you were given more time with this dataset, where do you think you would spend more time?
If given more time, I would focus on more advanced feature engineering, 
like creating interaction features between weather conditions and time of day. Additionally, 
I would experiment with deeper hyperparameter tuning or stacking models more effectively using different AutoGluon strategies.


### Create a table with the models you ran, the hyperparameters modified, and the kaggle score.
	model|	hpo1|	hpo2|	hpo3|	score|
0|	initial|	default_values|	default_values|	standard|	1.80473|
1|	add_features|	default_values|	default_values|	standard|	0.44976|
2|	hpo|	GBM| RF|	optimized_deployment|	0.47013|


### Create a line plot showing the top model score for the three (or more) training runs during the project. |



![model_train_score.png](img/model_train_score.png)

### Create a line plot showing the top kaggle score for the three (or more) prediction submissions during the project.



![model_test_score.png](img/model_test_score.png)

## Summary
After initial training, I noticed improvements when adding temporal and environmental features, 
and performance further improved through hyperparameter tuning. 
The best model turned out to be a weighted ensemble (WeightedEnsemble_L3) generated by AutoGluon, 
demonstrating the power of ensembling multiple algorithms.
The project provided insights into the importance of feature engineering and optimization when working with time-series-like data such as bike-sharing demand.
